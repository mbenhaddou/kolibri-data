{
        "LogisticRegression": {
        "class": "sklearn.linear_model.LogisticRegression",
        "description" : "In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.",
        "parameters": {
      "penalty": {
        "value": "l2",
        "type" : "categorical",
        "values": ["l1", "l2", "None", "elasticne"]
      },
      "class_weight":  {
        "value": null,
        "type": "categorical",
        "values": ["balanced",  "None"]
      },
      "solver": {
        "value":"lbfgs" ,
        "type": "categorical",
        "values":["newton-cg", "lbfgs", "liblinear"]
      },
      "multi_class": {
        "value": "auto",
        "type": "categorical",
        "values": ["auto", "ovr", "multinomial"]
      },
      "max_iter": {
        "value": 100,
        "type": "integer",
        "values": [200, 2000]
      },
      "C": {
        "value": 1.0,
        "type": "float",
        "values": [0.1, 10]
      }
    },
        "name": "logistic_regression",
        "family": "linear",
        "matrix": "sparse",
        "performance": "fast",
          "library": "sklearn"
  },
      "KNeighborsClassifier": {
        "class": "sklearn.neighbors.KNeighborsClassifier",
        "description" : "K-nearest neighbors (KNN) is a type of supervised learning algorithm used for both regression and classification. KNN tries to predict the correct class for the test data by calculating the distance between the test data and all the training points. Then select the K number of points which is closet to the test data. The KNN algorithm calculates the probability of the test data belonging to the classes of ‘K’ training data and class holds the highest probability will be selected. In the case of regression, the value is the mean of the ‘K’ selected training points.",
        "parameters": {
          "n_neighbors": {
            "value": 5,
            "type": "integer",
            "values": [ 3, 20]
         },
          "weights": {
            "value": "uniform",
            "type": "categorical",
            "values": [
              "uniform",
              "distance"
            ]
          },
          "algorithm": {
            "value": "auto",
            "type": "categorical",
            "values": [
              "auto",
              "ball_tree",
              "kd_tree",
              "brute"
            ]
          }
        },
        "name": "knn",
        "family":"neighbors",
        "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
      },
       "RadiusNeighborsClassifier": {
        "class": "sklearn.neighbors.RadiusNeighborsClassifier",
        "description" : "Radius Neighbors Classifier is a classification machine learning algorithm. It is an extension to the k-nearest neighbors algorithm that makes predictions using all examples in the radius of a new example rather than the k-closest neighbors. As such, the radius-based approach to selecting neighbors is more appropriate for sparse data, preventing examples that are far away in the feature space from contributing to a prediction.",
        "parameters": {
          "algorithm": {
            "value": "auto",
            "type": "categorical",
            "values": [
              "auto",
              "ball_tree",
              "kd_tree",
              "brute"
            ]
          },
          "radius": {
            "value": 3,
            "type": "float",
            "values": [
              1, 10
            ]
          }
        },
        "name": "radius_nn_classifier",
        "family":"neighbors",
          "matrix": "sparse",
        "performance": "fast",
         "library": "sklearn"
      },
      "MLPClassifier": {
          "class": "sklearn.neural_network.MLPClassifier",
          "description" : "MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.",
          "parameters": {
            "hidden_layer_sizes": {
              "value": 100,
              "type": "integer",
              "values": [
                50,
                100,
                150,
                200
              ]
            },
            "activation": {
              "value": "relu",
              "type": "categorical",
              "values": [
                "identity",
                "logistic",
                "tanh",
                "relu"
              ]
            },
            "solver": {
              "value": "adam",
              "type": "categorical",
              "values": [
                "adam",
                "lbfgs",
                "sgd"
              ]
            },
            "learning_rate_init": {
              "value": 0.001,
              "type": "float",
              "values": [0.0001, 0.001, 0.01]
            }
          },
          "name": "mlp",
        "family": "neural_network",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
        },
      "SVC": {
          "class": "sklearn.svm.SVC",
          "description" : "The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using LinearSVC or SGDClassifier instead, possibly after a Nystroem transformer. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.",
          "parameters": {
            "C": {
              "value": 1.0,
              "type": "float",
              "range": [
                0,
                2.0
              ]
            },
            "kernel": {
              "value": "rbf",
              "type": "categorical",
              "values": [
                "linear",
                "poly",
                "rbf",
                "sigmoid"
              ]
            },
            "degree": {
              "value": 3,
              "type": "integer",
              "values": [
                1, 2, 3, 4
              ]
            },
            "gamma": {
              "value": "scale",
              "type": "categorical",
              "values": ["scale", "auto"]
            },
            "probability": {
              "value": true
            }
          },
          "name": "svm",
        "family": "svm",
         "matrix": "sparse",
        "performance": "slow",
        "library": "sklearn"
        },
      "LinearSVC": {
          "class": "sklearn.svm.LinearSVC",
          "description" : "Similar to SVM with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.",
          "parameters": {
            "C": {
              "value": 1.0,
              "type": "float",
              "range": [
                0,
                2.0
              ]
            },
            "dual": {
              "value": true,
              "type": "integer",
              "values": [
                true, false
              ]
            }
          },
          "name": "linear_svc",
        "family": "svm",
         "matrix": "sparse",
        "performance": "medium",
        "library": "sklearn"
        },
      "MultinomialNB": {
          "class": "sklearn.naive_bayes.MultinomialNB",
          "description" : "Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.",
          "name": "multinomial",
          "parameters": {
            "alpha": {
              "value": 1.0,
              "type": "float",
              "range": [
                0,
                2.0
              ]
            }
          },
          "family": "probabilistic",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
        },
      "SGDClassifier": {
          "class": "sklearn.linear_model.SGDClassifier",
          "description" : "Linear classifiers (SVM, logistic regression, etc.) with SGD training. This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.",
         "name": "sgd_classifier",
          "parameters": {
            "loss": {
              "value": "hinge",
              "type": "categorical",
              "range": [
                "hinge", "log", "modified_huber", "squared_hinge", "perceptron"
              ]
            },
            "penalty": {
              "value": "l2",
              "type": "categorical",
              "range": [  "l1", "l2","elasticnet"  ]
            },
            "n_jobs": {
              "value": -1
            }
          },
        "family": "linear",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
        },
        "RandomForestClassifier": {
          "class": "sklearn.ensemble.RandomForestClassifier",
          "description" : "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.",
          "name": "randomforest",
          "parameters": {
            "n_estimators": {
              "value": 100,
              "type": "integer",
              "range": [
                10,
                200
              ]
            },
            "criterion": {
              "value": "gini",
              "type": "categorical",
              "values": [
                "gini",
                "entropy"
              ]
            },
            "max_depth": {
              "value": null,
              "type": "integer",
              "values": [
                null,1, 2, 3, 4
              ]
            },
            "min_samples_split": {
              "value": 2,
              "type": "float",
              "range": [0, 1]
            },
            "max_features": {
              "value": "auto",
              "type": "categorical",
              "values": [
                "auto",
                "sqrt",
                "log2"
              ]
            },
            "n_jobs": {
              "value": -1
            },
            "class_weight": {
              "value": null,
              "type": "categorical",
              "values": [
                "balanced",
                "balanced_subsample",
                null
              ]
            }
          },
        "family": "ensemble_tree",
         "matrix": "sparse",
        "performance": "fast",
          "library": "sklearn"
        },
      "DecisionTreeClassifier": {
          "class": "sklearn.tree.DecisionTreeClassifier",
          "description" : "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.",
           "name": "decision_tree",
          "parameters": {
            "criterion": {
              "value": "gini",
              "type": "categorical",
              "values": [
                "gini",
                "entropy"
              ]
            },
            "splitter": {
              "value": "best",
              "type": "categorical",
              "values": [
                "best", "random"
              ]
            },
            "max_depth": {
              "value": null,
              "type": "integer",
              "range": [0, 4]
            },
            "max_features": {
              "value": "auto",
              "type": "categorical",
              "values": [
                "auto",
                "sqrt",
                "log2"
              ]
            },
            "class_weight": {
              "value": null,
              "type": "categorical",
              "values": [
                "balanced",
                "balanced_subsample",
                null
              ]
            }
          },
        "family": "tree",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
        },
      "XGBClassifier": {
          "class": "xgboost.XGBClassifier",
          "description" : "XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.",
          "name": "xgboost",
          "parameters": {
            "booster": {
              "value": "gbtree",
              "type": "categorical",
              "values": [
                "gbtree", "gblinear", "dart"
              ]
            },
            "eta": {
              "value": 0.3,
              "type": "float",
              "range": [
                0,
                1
              ]
            },
            "gamma": {
              "value": 0,
              "type": "integer",
              "values": [
                0, 200
              ]
            },
            "max_depth": {
              "value": 6,
              "type": "integer",
              "range": [3, 9]
            },
            "subsample": {
              "value": 1.0,
              "type": "float",
              "range": [
                0,
                1.0
              ]
            },
            "tree_method": {
              "value": "auto",
              "type": "categorical",
              "values": [
                "auto",
                "exact",
                "approx",
                "hist"
              ]
            },
            "nthread": {
              "value": -1
            },
            "num_parallel_tree": {
              "value": 1,
              "type": "integer",
              "range": [1, 3]
            },
            "grow_policy": {
              "value":"depthwise",
              "type": "categorical",
              "values": [
                "lossguide",
                "depthwise"
              ]
            }
          },
            "family": "ensemble_tree",
           "matrix": "sparse",
            "performance": "slow",
        "library": "xgboost"
        },
      "EcocEstimator": {
        "class": "kolibri.backend.sklearn.meta.EcocEstimator",
          "name": "ecoc",
        "family": "ensemble",
         "matrix": "sparse",
        "performance": "medium",
        "library": "kolibri",
        "parameters": {
          "code_matrix": {
              "value": "rand",
              "type": "categorical",
              "values": []
            },
          "base_estimator": {
              "value": "LogisticRegression",
              "type": "categorical",
              "values": ["LogisticRegression", "KNeighborsClassifier", "MLPClassifier"]
          },
          "distance": {
              "value": "ecludian",
              "type": "categorical",
              "values": ["ecludian", "hamming"]
          }
        }
      },
      "Perceptron": {
      "class": "sklearn.linear_model.Perceptron",
      "description" : "The perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class",
      "parameters": {
        "penalty": {
        "value": "l2",
        "type" : "categorical",
        "values": ["l1", "l2", "elasticnet"]
      },
        "l1_ratio": {
        "value": 0.15,
        "type" : "float",
        "values": [0, 1]
      },
        "n_jobs": {
        "value": -1
      }
      },
      "name": "perceptron",
        "family": "linear",
       "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "PassiveAggressiveClassifier": {
      "class": "sklearn.linear_model.PassiveAggressiveClassifier",
      "description" : "Passive-aggressive classification is one of the available incremental learning algorithms and it is very simple to implement, since it has a closed-form update rule.The core concept is that the classifier adjusts its weight vector for each misclassified training sample it receives, trying to correct it.",
      "parameters": {
        "C": {
        "value": 1.0,
        "type" : "float",
        "values": [0, 5]
      },
        "class_weight": {
        "value": null,
        "type" : "categorical",
        "values": [ "balanced", "none"]
      },
        "n_jobs": {
        "value": -1
      }
      },
      "name": "passive_aggressive_classier",
        "family":"linear",
      "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "LabelPropagation":  {
        "class": "sklearn.semi_supervised.LabelPropagation",
        "description" : "Label propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications)",
        "parameters": {
          "gamma": {
          "value": 20,
          "type" : "integer",
          "values": [10, 50]
        },
          "n_neighbors": {
          "value": 7,
          "type" : "integer",
          "values": [3, 21]
        },
          "n_jobs": {
          "value": -1
        }
        },
        "name": "label_propagation",
        "family": "semi_supervised",
         "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "LabelSpreading": {
        "class": "sklearn.semi_supervised.LabelSpreading",
        "description" : "LabelSpreading model for semi-supervised learning. This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.",
        "parameters": {
          "gamma": {
            "value": 20,
            "type" : "integer",
            "values": [10, 50]
        },
          "n_neighbors": {
          "value": 7,
          "type" : "integer",
          "values": [3, 21]
        },
          "alpha": {
          "value": 0.2,
          "type" : "float",
          "values": [0, 1]
        },
          "n_jobs": {
          "value": -1
        }
        },
        "name": "label_spreading",
        "family": "semi_supervised",
         "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "GradientBoostingClassifier": {
        "class": "sklearn.ensemble.GradientBoostingClassifier",
        "description" : "Gradient Boosting builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.",
        "parameters": {
          "loss": {
            "value": "deviance",
            "type": "categorical",
            "values": [
              "deviance",
              "exponential"
            ]
          },
          "n_estimators": {
            "value": 100,
            "type": "integer",
            "values": [ 50, 1000]
          },
          "max_features": {
            "value": "auto",
            "type": "categorical",
            "values": [
              "auto",
              "sqrt",
              "log2"
            ]
          },
          "criterion": {
            "value": "friedman_mse",
            "type": "categorical",
            "values": [
              "friedman_mse",
              "mse",
              "mae"
            ]
          }
        },
          "name": "gradient_boosting_classifier",
        "family": "ensemble",
        "matrix": "sparse",
        "performance": "slow",
        "library": "sklearn"
        },
      "QuadraticDiscriminantAnalysis": {
        "class": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis",
        "description" : "A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class.",
        "parameters": {
          "reg_param": {
          "value": 0.0,
          "type" : "float",
          "values": [0, 1.0]
        }
        },
        "name": "quadratic_discriminant_analysis",
        "family": "discriminant_analysis",
        "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "HistGradientBoostingClassifier": {
        "class": "sklearn.ensemble.HistGradientBoostingClassifier",
        "description" : "This estimator is much faster than Gradient Boosting Classifier for big datasets (n_samples >= 10 000). This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.",
        "parameters": {
          "learning_rate": {
          "value": 0.1,
          "type" : "float",
          "values": [0, 1]
        }
        },
        "name": "hist_radient_boosting_classifier",
        "family": "ensemble",
         "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "RidgeClassifier": {
        "class": "sklearn.linear_model.RidgeClassifier",
        "description" : "This classifier first converts the target values into {-1, 1} and then treats the problem as a regression task (multi-output regression in the multiclass case).",
        "parameters": {
          "alpha": {
          "value": 1.0,
          "type" : "float",
          "values": [0, 6]
        },
          "class_weight": {
          "value": null,
          "type" : "categorical",
          "values": ["balanced", "none"]
        },
          "solver": {
          "value": "auto",
            "type" : "categorical",
          "values": ["svd", "auto", "cholesky", "lsqr", "sparse_cg", "sag", "saga"]
        }
        },
        "name": "ridge_classifier",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "ExtraTreesClassifier": {
        "class": "sklearn.ensemble.ExtraTreesClassifier",
        "description" : "This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.",
        "parameters": {
          "criterion": {
          "value": "gini",
          "type" : "categorical",
          "values": ["gini", "entropy"]
        },
          "n_estimators": {
          "value": 100,
          "type" : "integer",
          "values": [50, 1000]
        },
          "n_jobs": {
          "value": -1
        }
        },
        "name": "label_propagation",
        "family": "ensemble_tree",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"

  },
      "BernoulliNB": {
        "class": "sklearn.naive_bayes.BernoulliNB",
        "description" : "Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
        "parameters": {
          "alpha": {
          "value": 1.0,
          "type" : "float",
          "values": [0, 1]
        }
        },
        "name": "bernoulli_nb_classifier",
        "family": "probabilistic",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "LinearDiscriminantAnalysis": {
        "class": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
        "description" : "A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method.",
        "parameters": {
          "solver": {
          "value": "svd",
          "type" : "categorical",
          "values": ["svd", "lsqr", "eigen"]
        }
        },
        "name": "linear_descriminent_analysis",
        "family": "discriminant_analysis",
        "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "GaussianNB": {
        "class": "sklearn.naive_bayes.GaussianNB",
        "description" : "A Gaussian Naive Bayes algorithm is a special type of NB algorithm. It's specifically used when the features have continuous values. It's also assumed that all the features are following a gaussian distribution i.e, normal distribution.",
        "parameters": {
          "var_smoothing": {
          "value": 1e-9,
          "type" : "float",
          "values": [1e-10, 1e-5]
        }
        },
        "name": "gaussian_nb_classifier",
        "family": "probabilistic",
         "matrix": "dense",
        "performance": "fast",
        "library": "sklearn"
  },
      "NearestCentroid": {
        "class": "sklearn.neighbors.NearestCentroid",
        "description" : "Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.",
        "parameters": {
          "shrink_threshold": {
          "value": null,
          "type" : "float",
          "values": [1, 5]
        }
        },
        "name": "nearest_centroid",
        "family": "neighbors",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "ExtraTreeClassifier": {
        "class": "sklearn.tree.ExtraTreeClassifier",
        "description" : "Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.",
        "parameters": {
          "criterion": {
          "value": "gini",
          "type" : "categorical",
          "values": ["gini", "entropy"]
        },
          "splitter": {
          "value": "random",
          "type" : "categorical",
          "values": ["random", "best"]
        }
        },
        "name": "extra_tree_classifier",
        "family": "tree",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },
      "DummyClassifier": {
        "class": "sklearn.dummy.DummyClassifier",
        "description" : "This classifier serves as a simple baseline to compare against other more complex classifiers. The specific behavior of the baseline is selected with the strategy parameter. All strategies make predictions that ignore the input feature values passed as the X argument to fit and predict. The predictions, however, typically depend on values observed in the y parameter passed to fit. Note that the “stratified” and “uniform” strategies lead to non-deterministic predictions that can be rendered deterministic by setting the random_state parameter if needed. The other strategies are naturally deterministic and, once fit, always return a the same constant prediction for any value of X.",
        "parameters": {
          "strategy": {
          "value": "prior",
          "type" : "categorical",
          "values": ["stratified", "most_frequent", "prior", "uniform", "constant"]
        }
        },
        "name": "dummy_classifier",
        "family": "rule_based",
         "matrix": "sparse",
        "performance": "fast",
        "library": "sklearn"
  },

    "CatBoostClassifier": {
        "class": "catboost.CatBoostClassifier",
        "description" : "CatBoost is a machine learning algorithm that uses gradient boosting on decision trees. It is available as an open source library.",
        "parameters": {
        },
        "name": "catboost_classifier",
        "family": "tree_based",
         "matrix": "sparse",
        "performance": "fast",
      "library": "catboost"
  }
}